{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Lenovo\\Downloads\\Restaurant_Reviews.tsv\"\n",
    "data = pd.read_csv(file_path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords and lemmatize each token\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(lemmatized_tokens), lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cleaned_Review'], data['Lemmatized_Tokens'] = zip(*data['Review'].apply(preprocess_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0                           Wow... Loved this place.   \n",
      "1                                 Crust is not good.   \n",
      "2          Not tasty and the texture was just nasty.   \n",
      "3  Stopped by during the late May bank holiday of...   \n",
      "4  The selection on the menu was great and so wer...   \n",
      "\n",
      "                                      Cleaned_Review  \\\n",
      "0                                    wow loved place   \n",
      "1                                         crust good   \n",
      "2                                tasty texture nasty   \n",
      "3  stopped late may bank holiday rick steve recom...   \n",
      "4                         selection menu great price   \n",
      "\n",
      "                                   Lemmatized_Tokens  \n",
      "0                                [wow, loved, place]  \n",
      "1                                      [crust, good]  \n",
      "2                            [tasty, texture, nasty]  \n",
      "3  [stopped, late, may, bank, holiday, rick, stev...  \n",
      "4                    [selection, menu, great, price]  \n"
     ]
    }
   ],
   "source": [
    "print(data[['Review', 'Cleaned_Review', 'Lemmatized_Tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  \\\n",
      "995  I think food should have flavor and texture an...   \n",
      "996                           Appetite instantly gone.   \n",
      "997  Overall I was not impressed and would not go b...   \n",
      "998  The whole experience was underwhelming, and I ...   \n",
      "999  Then, as if I hadn't wasted enough of my life ...   \n",
      "\n",
      "                                        Cleaned_Review  \\\n",
      "995                  think food flavor texture lacking   \n",
      "996                            appetite instantly gone   \n",
      "997                    overall impressed would go back   \n",
      "998  whole experience underwhelming think well go n...   \n",
      "999  hadnt wasted enough life poured salt wound dra...   \n",
      "\n",
      "                                     Lemmatized_Tokens  \n",
      "995            [think, food, flavor, texture, lacking]  \n",
      "996                        [appetite, instantly, gone]  \n",
      "997              [overall, impressed, would, go, back]  \n",
      "998  [whole, experience, underwhelming, think, well...  \n",
      "999  [hadnt, wasted, enough, life, poured, salt, wo...  \n"
     ]
    }
   ],
   "source": [
    "print(data[['Review', 'Cleaned_Review', 'Lemmatized_Tokens']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
